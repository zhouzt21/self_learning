# Course Goals

## Lecture 1 2

1. Define MP, MRP, MDP, Bellman operator, contraction, model, Q-value, policy
2. Be able to implement: **[hw]** 
    - `Value Iteration`
    - `Policy Iteration`
3. Give pros and cons of different policy evaluation approaches
4. Be able to prove contraction properties Limitations of presented approaches and Markov assumptions
    - Which policy evaluation methods require the Markov assumption?

## Lecture 3 4

1. Be able to implement `TD(0)` and `MC` on policy evaluation **[hw]** 
2. Be able to implement `Q-learning` and `SARSA` and `MC control` algorithms **[hw]** 
3. List the 3 issues that can cause instability and describe the problems qualitatively:
   - function approximation, bootstrapping and off-policy learning
4. Know some of the key features in `DQN` that were critical
   - experience replay, fixed targets

## Lecture 5 6 7