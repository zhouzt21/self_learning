{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16df9564",
   "metadata": {},
   "source": [
    "# Lecture 12 - Fast RL II\n",
    "\n",
    "provided by [Stanford CS234](https://www.youtube.com/watch?v=FgzM3zpZ55o)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c06cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Table of Contents: <br>\n",
    "    \n",
    "<ul>\n",
    "    <li>1. <a href=\"#1.-Introduction\">Introduction</a>\n",
    "    <li>2. <a href=\"#2.-Bayesian-Bandits\">Bayesian Bandits</a></li>\n",
    "    <li>3. <a href=\"#3.-Probability-Matching\">Probability Matching</a></li>\n",
    "    <li>4. <a href=\"#4.-Framework:-Probably-Approximately-Correct\">Framework: Probably Approximately Correct</a></li>\n",
    "    <li>5. <a href=\"#5.-Fast-RL-in-MDPs\">Fast RL in MDPs</a></li>\n",
    "    <li>6. <a href=\"#6.-Resource\">Resource</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8837110",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679241e8",
   "metadata": {},
   "source": [
    "We have been covering algorithms that fall under the concept of __optimism under uncertainty__. \n",
    "\n",
    "We have looked at the following approaches:\n",
    "* __Greedy__: linear total regret\n",
    "* __Constant $\\epsilon$-greedy__: linear total regret\n",
    "* __Decaying $\\epsilon$-greedy__: sublinear regret\n",
    "* __UCB__: sublinear regret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9035e",
   "metadata": {},
   "source": [
    "# 2. Bayesian Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42491d95",
   "metadata": {},
   "source": [
    "Before in UCB, we made no assumptions about the unknown reward distribution $R$ except for the bounds on the rewards.\n",
    "\n",
    "Another approach is called __bayesian bandits__ and exploits prior knowledge of rewards $p[R]$. It computes a posterior distribution of rewards $p[R~|~h_{t}]$ based on a history of action reward pairs.\n",
    "\n",
    "It leverages Bayes' rule:\n",
    "\n",
    "$$\n",
    "p(\\phi_{i}~|~r_{i1}) = \\frac{p(r_{i1}~|~\\phi_{i})p(\\phi_{i})}{p(r_{i1})} \\hspace{1em} (Eq.~1)\\\\\n",
    "$$\n",
    "\n",
    "If $p(\\phi_{i}~|~r_{i1})$ and $p(\\phi_{i})$ are the same, then we can call the prior $p(\\phi_{i})$ and model $p(r_{i1}~|~\\phi_{i})$ a __conjugate__. Why is this useful? It means we can do our posterior updating analytically.\n",
    "\n",
    "Framework:\n",
    "* __frequentist regret__ : (the framework we have been using before) assumes a true unknown set of parameters\n",
    "\n",
    "$$\n",
    "Regret(\\mathcal{A}, T; \\theta) = \\sum_{t = 1}^{t} \\mathbb{E}[Q(a^{*}) - Q(a_{t})] \\hspace{1em} (Eq.~2)\\\\\n",
    "$$\n",
    "\n",
    "* __bayesian regret__ : assumes there's a prior over parameters\n",
    "\n",
    "$$\n",
    "BayesRegret(\\mathcal{A}, T; \\theta) = \\mathbb{E}_{\\theta \\sim p_{\\theta}} [\\sum_{t = 1}^{t} \\mathbb{E}[Q(a^{*}) - Q(a_{t})~|~\\theta]] \\hspace{1em} (Eq.~3)\\\\\n",
    "$$\n",
    "\n",
    "We tackle this framework with __probability matching__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26489369",
   "metadata": {},
   "source": [
    "# 3. Probability Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c73893",
   "metadata": {},
   "source": [
    "We assume we have a parametric distribution over rewards for each arm. \n",
    "\n",
    "__Probability Matching__ selects the best action (optimal action) based on a history.\n",
    "\n",
    "$$\n",
    "\\pi(a~|~h_{t}) = \\mathbb{P}[Q(a) > Q(a'), \\forall a' \\ne a ~|~ h_{t}] \\hspace{1em} (Eq.~4)\\\\\n",
    "$$\n",
    "\n",
    "Uncertain actions have higher probability of being max."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45caa0c",
   "metadata": {},
   "source": [
    "Initialize prior over each arm $a, p(R_{a})$ <br>\n",
    "loop <br>\n",
    "$\\quad$ For each arm $a$ _sample_ a reward distribution $R_{a}$ from posterior <br>\n",
    "$\\quad$ Compute action-value function $Q(a) = \\mathbb{E}[R_{a}]$ <br>\n",
    "$\\quad$ $a_{t} = \\underset{a \\in \\mathcal{A}}{argmax}Q(a)$ <br>\n",
    "$\\quad$ Observe reward $r$ <br>\n",
    "$\\quad$ Update posterior $p(R_{a}~|~r)$ using Bayes law <br>\n",
    "\n",
    "_Algorithm 1. Thompson Sampling._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16e1bb",
   "metadata": {},
   "source": [
    "I found this resource to be really helpful for understanding thompson sampling: https://www.youtube.com/watch?v=Zgwfw3bzSmQ.\n",
    "\n",
    "_Thompson sampling has the same regret bounds as UCB._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7c09a",
   "metadata": {},
   "source": [
    "# 4. Framework: Probably Approximately Correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ffd8d",
   "metadata": {},
   "source": [
    " Because we evaluate based on total regret, we don't know if regret is caused by a lot of little mistakes or a few large ones.\n",
    " \n",
    " We can tackle this problem with the __Probably Approximately Correct (PAC)__ framework.\n",
    " \n",
    " $$\n",
    " Q(a) \\ge Q(a^{*}) - \\epsilon \\hspace{1em} (Eq.~5)\\\\\n",
    " $$\n",
    " \n",
    " Basically it will operate much like before (optimism or Thompson sampling) however a small $\\epsilon$ is added to give room for other actions to be selected.\n",
    " \n",
    "From what I'm understanding, this framework can be applied to optimism under uncertainty and probability matching/thompson sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae85ac",
   "metadata": {},
   "source": [
    "# 5. Fast RLs in MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad3cc8",
   "metadata": {},
   "source": [
    "For the MDP setting (we've been covering the multi-armed bandit setting), we can use the same frameworks. This section focuses on the PAC framework.\n",
    "\n",
    "Not too sure, but from what I understand, I would think UCB and Thompson sampling are only applicable to the multi-armed bandit setting. In the (tabular) MDP setting, they carry the same ideas but aren't exactly the same.\n",
    "\n",
    "The lecture begins with __optimistic initialization__. In the MDP setting, we can use any of the model-free algorithms (e.g. SARSA, MC, Q-learning) we've learned to estimate $Q(s, a)$. \n",
    "\n",
    "We can initialize our q-values optimistically like setting them to $\\frac{r_{max}}{1 - \\gamma}$ or initializing $V(s) = \\frac{r_{max}}{(1 - \\gamma) \\Pi_{i=1}^{T} \\alpha_{i}}$. We consider $r_{max}$ to be the state-action pair that maximizes the reward. $\\gamma$ is the discount factor. $\\alpha_{i}$ is the learning rate at the $i$-th timestep which goes up till $T$, the number of samples to learn near optimal q-values.\n",
    "\n",
    "Optimistic initialization is one way to make RL faster in the MDP setting.\n",
    "Other approaches include:\n",
    "* be very optimistic till confident empirical estimates close to true parameters\n",
    "* be optimistic given information you have\n",
    "    * compute confidence sets on dynamics/reward models\n",
    "    * add reward bonuses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda8aa3",
   "metadata": {},
   "source": [
    "Given $\\epsilon, \\delta, m$ <br>\n",
    "$\\beta = \\frac{1}{1 - \\gamma} \\sqrt{0.5 ln(2|S||A|\\frac{m}{\\delta})}$ <br>\n",
    "$n_{sas}(s, a, s') = 0; s \\in S, a \\in A, s' \\in S$ <br>\n",
    "$rc(s, a) = 0, n_{sa}(s, a) = 0, \\tilde{Q}(s, a) = \\frac{1}{1 - \\gamma} \\forall s \\in S, a \\in A$ <br>\n",
    "$t = 0; s_{t} = s_{init}$ <br>\n",
    "loop <br>\n",
    "$\\quad$ $a_{t} = \\underset{a \\in A}{argmax} Q(s_{t}, a)$ <br>\n",
    "$\\quad$ Observe reward $r_{t}$ and state $s_{t + 1}$ <br>\n",
    "$\\quad$ $n_{sa}(s_{t}, a_{t}) += 1$ <br>\n",
    "$\\quad$ $n_{sas}(s_{t}, a_{t}, s_{t + 1}) += 1$ <br>\n",
    "$\\quad$ $rc(s_{t}, a_{t}) = \\frac{rc(s_{t}, a_{t})n_{sa}(s_{t}, a_{t}) + r_{t}}{n_{sa}(s_{t}, a_{t}) + 1}$ <br>\n",
    "$\\quad$ $\\hat{R}(s, a) = \\frac{rc(s_{t}, a_{t})}{n(s_{t}, a_{t})}$ <br>\n",
    "$\\quad$ $\\hat{T}(s'~|~s, a) = \\frac{n_{sas}(s_{t}, a_{t}, s_{t + 1})}{n_{sa}(s_{t}, a_{t})} \\forall s' \\in S$ <br>\n",
    "$\\quad$ while not converged do <br>\n",
    "$\\quad\\quad$ $\\hat{Q}(s, a) = \\hat{R}(s, a) + \\gamma \\sum_{s'}\\hat{T}(s'~|~s, a)\\underset{a'}{max}\\tilde{Q}(s', a') + \\underbrace{\\frac{\\beta}{\\sqrt{n_{sa}(s, a)}}}_{reward ~ bonus} \\forall s \\in S, a \\in A$\n",
    "\n",
    "_Algorithm 2. Model-Based Interval Estimation with Exploration Bonus (MBIE-EB)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95799536",
   "metadata": {},
   "source": [
    "Algorithm 2 (MBIE-EB) uses value iteration for model-based policy control (but it estimates the reward and dynamics models). It also implements an exploration bonus (or reward bonus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f657f6b",
   "metadata": {},
   "source": [
    "# 6. Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e04a27",
   "metadata": {},
   "source": [
    "If you missed the link right below the title, I'm providing the resource here again along with the course website.\n",
    "\n",
    "- [Stanford CS234](https://www.youtube.com/watch?v=FgzM3zpZ55o)\n",
    "- [Course Website](http://web.stanford.edu/class/cs234/index.html)\n",
    "\n",
    "This is a series of 15 lectures provided by Stanford.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c15c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
