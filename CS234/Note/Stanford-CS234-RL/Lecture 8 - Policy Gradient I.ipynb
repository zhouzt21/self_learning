{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16df9564",
   "metadata": {},
   "source": [
    "# Lecture 8 - Policy Gradient I\n",
    "\n",
    "provided by [Stanford CS234](https://www.youtube.com/watch?v=FgzM3zpZ55o)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c06cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Table of Contents: <br>\n",
    "    \n",
    "<ul>\n",
    "    <li>1. <a href=\"#1.-Introduction\">Introduction</a>\n",
    "    <li>2. <a href=\"#2.-Policy-Optimization\">Policy Optimization</a></li>\n",
    "    <li>3. <a href=\"#3.-Resource\">Resource</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8837110",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803854f9",
   "metadata": {},
   "source": [
    "In the previous lecture, we tried to approximate the value or action-value functions using parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "V_{\\theta}(s) \\approx V^{\\pi}(s)\\\\\n",
    "Q_{\\theta}(s, a) \\approx Q^{\\pi}(s, a)\n",
    "$$\n",
    "\n",
    "The policy will usually be $\\epsilon$-greedy applied on top of these value functions. Today we will directly parameterize the policy:\n",
    "\n",
    "$$\n",
    "\\pi_{\\theta}(s, a) = \\mathbb{P}[a~|~s; \\theta] \\hspace{1em} (Eq.~1)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abfa2c7",
   "metadata": {},
   "source": [
    "* Value Based\n",
    "    * Learnt value function\n",
    "    * implicit policy (e.g. $\\epsilon$-greedy)\n",
    "* Policy Based\n",
    "    * no value function\n",
    "    * learnt policy\n",
    "* Actor-Critic\n",
    "    * learnt value function\n",
    "    * learnt policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1190870",
   "metadata": {},
   "source": [
    "Advantages of Policy-Based RL:\n",
    "* better convergence properties\n",
    "* effective in high-dimensional/continuous action spaces\n",
    "Disadvantages:\n",
    "* usually converge to local rather than global optimum\n",
    "* evaluating policy is inefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13807f37",
   "metadata": {},
   "source": [
    "* Goal: given a policy $\\pi_{\\theta}(s, a)$ with parameters $\\theta$, find best $\\theta$\n",
    "* we measure the quality of the policy (policy evaluation)\n",
    "* in __episodic environments__, we can use the start value of the policy:\n",
    "\n",
    "$$\n",
    "J_{1}(\\theta) = V^{\\pi_{\\theta}}(s_{1}) \\hspace{1em} (Eq.~2)\\\\\n",
    "J_{avV}(\\theta) = \\sum_{s} d^{\\pi_{\\theta}}(s) V^{\\pi_{\\theta}}(s) \\hspace{1em} (Eq.~3)\\\\\n",
    "J_{avR}(\\theta) = \\sum_{s} d^{\\pi_{\\theta}}(s) \\sum_{a} \\pi_{\\theta}(s, a) R(a, s) \\hspace{1em} (Eq.~4)\\\\\n",
    "$$\n",
    "\n",
    "$d^{\\pi_{\\theta}}(s)$ is the stationary distribution of states under $\\pi_{\\theta}$.\n",
    "\n",
    "Eq. 2: in episodic environments we can use the start value of the policy state $s_{1}$. Eq. 3: in continuing environments we can use the average value. Eq. 4: in continuing environments we can also use the average reward per time-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12637991",
   "metadata": {},
   "source": [
    "# 2. Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70e469",
   "metadata": {},
   "source": [
    "Policy-based RL (we've been doing model-based/model-free value-based RL) is an optimization problem. There are gradient-free methods for optimization:\n",
    "* hill climbing\n",
    "* genetic algorithms\n",
    "\n",
    "Non-gradient optimization methods are good baselines, but they are sample inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d07646",
   "metadata": {},
   "source": [
    "Policy gradient algorithms search for a _local_ maximum in $V(\\theta)$. There are many different PG algorithms.\n",
    "\n",
    "$$\n",
    "V(\\theta) = V^{\\pi_{\\theta}}\\\\\n",
    "\\Delta \\theta = \\alpha \\nabla_{\\theta}V(\\theta)\\\\\n",
    "\\nabla_{\\theta}V(\\theta) = s_{t} = \\begin{pmatrix}\n",
    "    \\frac{\\partial V(\\theta)}{\\partial \\theta_{1}} \\\\\n",
    "    \\vdots \\\\\n",
    "\t\\frac{\\partial V(\\theta)}{\\partial \\theta_{n}}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$\\alpha$ is a step-size parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ef4d8",
   "metadata": {},
   "source": [
    "__PG by Finite Differences__ is simple, noisy, and inefficient, but can sometimes be good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694da25",
   "metadata": {},
   "source": [
    "To evaluate policy gradient of $\\pi_{\\theta}(s, a)$ <br>\n",
    "For each dimension $k \\in [1, n]$ <br>\n",
    "$\\quad$ Estimate $k$-th partial derivative of objective function w.r.t. $\\theta$ <br>\n",
    "$\\quad$ Perturb $\\theta$ by small amount $\\epsilon$ in $k$-th dimension\n",
    "\n",
    "$$\n",
    "\\frac{\\partial V(\\theta)}{\\partial \\theta_{k}} \\approx \\frac{V(\\theta + \\epsilon u_{k}) - V(\\theta)}{\\epsilon}\n",
    "$$\n",
    "\n",
    "$u_{k}$ is a unit vector with 1 in $k$-th component, 0 elsewhere. <br>\n",
    "\n",
    "_Algorithm 1. PG by Finite Differences._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ad3a5",
   "metadata": {},
   "source": [
    "__Likelihood Ratio Policies__\n",
    "\n",
    "Define a state-action trajectory: $\\tau = (s_{0}, a_{0}, r_{0}, ..., s_{T - 1}, r_{T - 1}, s_{T})$ <br>\n",
    "Let $R(\\tau) = \\sum_{t=0}^{T}R(s_{t}, a_{t})$ be the sum of rewards for a trajectory $\\tau$. <br>\n",
    "The policy value is:\n",
    "\n",
    "$$\n",
    "V(\\theta) = \\sum_{\\tau} P(\\tau; \\theta) R(\\tau) \\hspace{1em} (Eq.~5)\\\\\n",
    "\\underset{\\theta}{argmax} V(\\theta) = \\underset{\\theta}{argmax} \\sum_{\\tau} P(\\tau; \\theta)R(\\tau) \\hspace{1em} (Eq.~6)\\\\\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\nabla_{\\theta}V(\\theta) & = \\sum_{\\tau} P(\\tau; \\theta) R(\\tau) \\nabla_{\\theta} log ~ P(\\tau;\\theta)\\\\\n",
    "    & = \\mathbb{E}_{\\tau}[\\sum_{t = 0}^{T - 1} \\nabla_{\\theta} log \\pi_{\\theta}(a_{t}~|~s_{t}) G_{t}^{(i)}]\\\\\n",
    "    & \\approx \\hat{g} =  (\\frac{1}{m}) \\sum_{i = 1}^{m} R(\\tau^{(i)}) \\nabla_{\\theta} log ~ P(\\tau;\\theta)\\\\\n",
    "    & = \\frac{1}{m} \\sum_{i = 1}^{m} R(\\tau^{(i)}) \\sum_{t = 0}^{T_{i}} \\nabla_{\\theta} log \\pi_{\\theta}(a_{t}~|~s_{t})\\\\\n",
    "    & = \\frac{1}{m} \\sum_{i = 1}^{m} \\sum_{t = 0}^{T - 1} \\nabla_{\\theta} log \\pi_{\\theta}(a_{t}~|~s_{t}) G_{t}^{(i)}\\\\\n",
    "    \\end{split}\n",
    "\\end{equation} \\hspace{1em} (Eq.~7)\\\\\n",
    "$$\n",
    "\n",
    "$P(\\tau; \\theta)$ is the probability over trajectories when executing policy $\\pi_{\\theta}$.\n",
    "\n",
    "In likelihood ratio policies, we often see value functions to be modeled like Eq. 5. Eq. 6 is a mathematical formulation for how we can optimize for a policy. Eq. 7 is the actual gradient of the value function w.r.t. the parameters of the policy $\\theta$. Notice how policy gradient algorithms directly optimize for the policy (and in this case, it optimizes via using gradients). The last 2 equations in Eq. 7 is the most important as those are the crux of REINFORCE, one classic policy gradient algorithm!\n",
    "\n",
    "Note: $log \\pi(a_{t}~|~s_{t}; \\theta)$ is the same as $log \\pi_{\\theta}(a_{t}~|~s_{t})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3658639",
   "metadata": {},
   "source": [
    "The rest of the lecture highlights the REINFORCE algorithm, one common policy gradient method.\n",
    "\n",
    "For an implementation of the algorithm I'd recommend this: https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb.\n",
    "\n",
    "For understanding it, I recommend this: https://medium.com/intro-to-artificial-intelligence/reinforce-a-policy-gradient-based-reinforcement-learning-algorithm-84bde440c816."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f657f6b",
   "metadata": {},
   "source": [
    "# 3. Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e04a27",
   "metadata": {},
   "source": [
    "If you missed the link right below the title, I'm providing the resource here again along with the course website.\n",
    "\n",
    "- [Stanford CS234](https://www.youtube.com/watch?v=FgzM3zpZ55o)\n",
    "- [Course Website](http://web.stanford.edu/class/cs234/index.html)\n",
    "\n",
    "This is a series of 15 lectures provided by Stanford.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c15c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
