{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16df9564",
   "metadata": {},
   "source": [
    "# Lecture 7 - Imitation Learning\n",
    "\n",
    "provided by [Stanford CS234](https://www.youtube.com/watch?v=FgzM3zpZ55o)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c06cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Table of Contents: <br>\n",
    "    \n",
    "<ul>\n",
    "    <li>1. <a href=\"#1.-Introduction\">Introduction</a>\n",
    "    <li>2. <a href=\"#2.-Problem-Setup\">Problem Setup</a>\n",
    "    <li>3. <a href=\"#3.-Behavioral-Cloning\">Behavioral Cloning</a>\n",
    "    <li>4. <a href=\"#4.-Inverse-RL\">Inverse RL</a>\n",
    "    <li>5. <a href=\"#5.-Apprenticeship-Learning\">Apprenticeship Learning</a>\n",
    "    <li>6. <a href=\"#6.-Resource\">Resource</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8837110",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0677a008",
   "metadata": {},
   "source": [
    "Some environments may have sparse rewards and even a DQN wouldn't be able to succeed in the environment. An example is Montezuma's Revenge which is a game where a character navigates a 2D world in order find a key and open a door.\n",
    "\n",
    "RL is good for simple and cheap data and parallelization is easy. However, it wouldn't be practical for cases where executing actions is slow, expensive to fail, or safety is priority. \n",
    "\n",
    "Problems with RL:\n",
    "* needs lots of data\n",
    "* needs lot of time\n",
    "* sparse rewards\n",
    "* hard to learn \n",
    "* execution of actions is slow\n",
    "* very expensive to fail\n",
    "* not safe \n",
    "\n",
    "__Imitation Learning__:\n",
    "* learn from imitating behavior\n",
    "* rewards are dense in time to closely guide the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296d6f9",
   "metadata": {},
   "source": [
    "# 2. Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fc797",
   "metadata": {},
   "source": [
    "Input:\n",
    "* state space, action space\n",
    "* Transition model $P(s' ~|~ s, a)$\n",
    "* No reward function $R$\n",
    "* Set of one or more teacher's demonstrations $(s_{0}, a_{0}, s_{1}, a_{1}, s_{2})$\n",
    "\n",
    "__Behavioral Cloning__:\n",
    "* Can we directly learn the teacher's policy using supervised learning?\n",
    "\n",
    "__Inverse RL__:\n",
    "* Can we recover $R$?\n",
    "\n",
    "__Apprenticeship Learning via Inverse RL__:\n",
    "* Can we use the R we find in Inverse RL to generate a good policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b5fd9",
   "metadata": {},
   "source": [
    "# 3. Behavioral Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0cab4",
   "metadata": {},
   "source": [
    "Behavioral Cloning:\n",
    "* the second your model deviates from the teacher behavior, it will have no idea what to do\n",
    "* fine so long as your data covers all possible states encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d185e6",
   "metadata": {},
   "source": [
    "Initialize $D \\leftarrow \\emptyset$ <br>\n",
    "Initialize $\\hat{\\pi}_{1}$ to any policy in $\\Pi$ <br>\n",
    "for $i = 1$ to $N$ do <br>\n",
    "$\\quad$ Let $\\pi_{i} = \\beta_{i}\\pi^{*} + (1 - \\beta_{i})\\hat{\\pi}_{i}$ <br>\n",
    "$\\quad$ Sample $T$-step trajectories using $\\pi_{i}$ <br>\n",
    "$\\quad$ Get dataset $D_{i} = \\{(s, \\pi^{*}(s))\\}$ of visited states by $\\pi_{i}$ and actions given by expert <br>\n",
    "$\\quad$ Aggregate datasets: $D \\rightarrow D \\cup D_{i}$ <br>\n",
    "$\\quad$ Train classifier $\\hat{\\pi}_{i + 1}$ on $D$. <br>\n",
    "\n",
    "Return best $\\hat{\\pi}_{i}$ on validation \n",
    "<br><br>\n",
    "\n",
    "_Algorithm 1. DAGGER: Dataset Aggregation._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b57242",
   "metadata": {},
   "source": [
    "The basic principle behind __DAGGER__ for behavior cloning is that you continually build up the dataset, train, and repeat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5c8ee",
   "metadata": {},
   "source": [
    "# 4. Inverse RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef0adf3",
   "metadata": {},
   "source": [
    "We have to estimate the $R$ reward function. There is no unique $R$ for a given set of data. \n",
    "\n",
    "$R(s) = \\textbf{w}^{T}x(s)$ where $w \\in \\mathbb{R}^{n}$, $x : S \\rightarrow \\mathbb{R}^{n} \\hspace{1em} (Eq.~1)$\n",
    "\n",
    "The value function for a policy $\\pi$ is now:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\begin{split}\n",
    "V^{\\pi} & \\underset{s \\thicksim \\pi}{=} \\mathbb{E}[\\sum_{t = 0}^{\\infty}\\gamma^{t}R(s_{t}) ~|~ \\pi]\\\\\n",
    "    & = \\mathbb{E}[\\sum_{t = 0}^{\\infty}\\gamma^{t}\\textbf{w}^{T}x(s_{t}) ~|~ \\pi]\\\\\n",
    "    & = \\textbf{w}^{T} \\mathbb{E}[\\sum_{t = 0}^{\\infty}\\gamma^{t}x(s_{t}) ~|~ \\pi]\\\\\n",
    "    & = \\textbf{w}^{T} \\mu(\\pi)\\\\\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\\hspace{1em} (Eq.~2)\\\\\n",
    "$$\n",
    "\n",
    "$\\mu(\\pi)(s)$ is the discounted weighted frequency of state features under policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9997dd8",
   "metadata": {},
   "source": [
    "# 5. Apprenticeship Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e70e7",
   "metadata": {},
   "source": [
    "$$\n",
    "V^{\\pi} = \\textbf{w}^{T} \\mu(\\pi)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\sum_{t = 0}^{\\infty}\\gamma^{t}R^{*}(s_{t}) ~|~ \\pi^{*}] = V^{*} \\ge V^{\\pi} = \\mathbb{E}[\\sum_{t = 0}^{\\infty}\\gamma^{t}R^{*}(s_{t}) ~|~ \\pi] \\hspace{1em} (Eq.~3)\\\\\n",
    "w^{*^{T}} \\mu(\\pi^{*}) \\ge w^{*^{T}} \\mu(\\pi), \\forall ~ \\pi \\ne \\pi^{*} \\hspace{1em} (Eq.~4)\\\\\n",
    "$$\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "||\\mu(\\pi) - \\mu(\\pi^{*})||_{1} \\le \\epsilon \\hspace{1em} (Eq.~5)\\\\\n",
    "$$\n",
    "\n",
    "then for all $w$ with $||w||_{\\infty} \\le 1$: <br><br>\n",
    "$$\n",
    "|w^{T}\\mu(\\pi) - w^{T}\\mu(\\pi^{*})| \\le \\epsilon \\hspace{1em} (Eq.~6)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba7e7e",
   "metadata": {},
   "source": [
    "Assumption: $R(s) = w^{T}x(s)$ <br>\n",
    "Initialize policy $\\pi_{0}$\n",
    "for $i = 1, 2, ...$ <br>\n",
    "$\\quad$ Find a reward function ($\\textbf{w}$) such that the teacher maximally outperforms all previous controllers:\n",
    "\n",
    "$$\n",
    "\\underset{\\textbf{w}}{argmax} ~ \\underset{\\gamma}{max} ~ s.t. ~~ w^{T} \\mu(\\pi^{*}) \\ge w^{T}\\mu(\\pi) + \\gamma ~~~ \\forall \\pi \\in \\{\\pi_{0}, \\pi_{1}, ..., \\pi_{i - 1}\\} ~~ s.t. ~~ ||w||_{2} \\le 1 \\hspace{1em} (Eq.~7)\\\\\n",
    "$$\n",
    "\n",
    "$\\quad$ Find optimal control policy $\\pi_{i}$ for the current $\\textbf{w}$ <br>\n",
    "$\\quad$ Exit if $\\gamma \\le \\frac{\\epsilon}{2}$ <br>\n",
    "\n",
    "_Algorithm 2. Apprenticeship Learning._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f657f6b",
   "metadata": {},
   "source": [
    "# 6. Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e04a27",
   "metadata": {},
   "source": [
    "If you missed the link right below the title, I'm providing the resource here again along with the course website.\n",
    "\n",
    "- [Stanford CS234](https://www.youtube.com/watch?v=FgzM3zpZ55o)\n",
    "- [Course Website](http://web.stanford.edu/class/cs234/index.html)\n",
    "\n",
    "This is a series of 15 lectures provided by Stanford.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c15c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
