# Stanford-CS234-RL---Lecture-Notes
My lecture notes on the RL series provided by Stanford.

## Table of Contents:
- [1. Description](https://github.com/alckasoc/Stanford-CS234-RL---Lecture-Notes/blob/main/README.md#1-description)
- [2. Difficulties](https://github.com/alckasoc/Stanford-CS234-RL---Lecture-Notes/blob/main/README.md#2-difficulties)
- [3. Author Info](https://github.com/alckasoc/Stanford-CS234-RL---Lecture-Notes/blob/main/README.md#3-author-info)
- [4. Thank You](https://github.com/alckasoc/Stanford-CS234-RL---Lecture-Notes/blob/main/README.md#4-thank-you)

## 1. Description

All lectures (and the images I took) are located in this repo. Below is a list of these corresponding notebooks in Kaggle:
* [Lecture 1](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-1)
* [Lecture 2](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-2)
* [Lecture 3](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-3)
* [Lecture 4](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-4)
* [Lecture 5](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-5)
* [Lecture 6](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-6)
* [Lecture 7](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-7)
* [Lecture 8](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-8)
* [Lecture 9](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-9)
* [Lecture 10](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-10)
* [Lecture 11](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-11)
* [Lecture 12](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-12)
* [Lecture 13](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-13)
* [Lecture 14](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-14)
* [Lecture 15](https://www.kaggle.com/code/vincenttu/stanford-cs234-rl-lecture-15)

## 2. Difficulties

  A few difficulties! Besides school, these lectures were very dense! Information was packed into the last few minutes of class too. I thought this was great (more material the better). I'm still a newbie in RL, but I think this lecture series, more than anything, provided me the landscape of modern RL and gave me insights into a a number of nooks and crannies. This lecture series definitely helped solidify my fundamental understanding of RL. I remember before I'd always mix up the different Bellman equation flavors. Why does one equation have states and actions while the other has only states? These small things would always stump me! These lectures certainly clarified all of that for me. I also think a lot of RL is theoretical! The math is definitely heavy. This made some parts of the lecture a bit more dense than usual. I ended up spending a lot of time going through these lectures, but it was all worth it!

## 3. Author Info

- Vincent Tu:            [LinkedIn](https://www.linkedin.com/in/vincent-tu-422b18208/) | [Kaggle](https://www.kaggle.com/vincenttu)

## 4. Thank You

This lecture series of notes took a while to compile (maybe a few months). These notes have helped me a ton in reviewing and understanding RL. I hope they also help you (do still watch the lectures!). I knew I was in for a very insightful look into RL when I noticed how much the first lecture covered. From model-based to model-free policy evaluation and control to value function approximation, deep learning, imitation learning, policy gradients, and fast and batch RL, I found the lectures to be informative and clear. I'd like to thank Professor Brunskill for making this series possible! I'd also like to thank everyone involved in helping with this lecture series and also you the viewer! Thank you.