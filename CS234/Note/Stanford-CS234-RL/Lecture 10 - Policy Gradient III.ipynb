{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16df9564",
   "metadata": {},
   "source": [
    "# Lecture 10 - Policy Gradient III\n",
    "\n",
    "provided by [Stanford CS234](https://www.youtube.com/watch?v=FgzM3zpZ55o)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c06cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Table of Contents: <br>\n",
    "    \n",
    "<ul>\n",
    "    <li>1. <a href=\"#1.-Introduction\">Introduction</a>\n",
    "    <li>2. <a href=\"#2.-Need-for-Automatic-Step-Size-Tuning\">Need for Automatic Step Size Tuning</a>\n",
    "    <ul>\n",
    "        <li>2.1. <a href=\"#2.1.-Local-Approximation\">Local Approximation</a></li>\n",
    "        <li>2.2. <a href=\"#2.2.-Trust-Region\">Trust Region</a></li>\n",
    "        <li>2.3. <a href=\"#2.3.-TRPO\">TRPO</a></li>\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li>3. <a href=\"#3.-Resource\">Resource</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8837110",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1a301",
   "metadata": {},
   "source": [
    "Today's lecture will cover the 2 other methods for automatic step-size tuning: trust regions and the TRPO algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f36cf",
   "metadata": {},
   "source": [
    "# 2. Need for Automatic Step Size Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a14379",
   "metadata": {},
   "source": [
    "Recall the objective function we defined in Lecture 9.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    L_{\\pi}(\\tilde{\\pi}) = V(\\tilde{\\theta}) & = V(\\theta) + \\mathbb{E}_{\\pi_{\\tilde{\\theta}}}[\\sum_{t = 0}^{\\infty} \\gamma^{t} A_{\\pi}(s_{t}, a_{t})]\\\\\n",
    "    & = V(\\theta) + \\sum_{s} \\mu_{\\tilde{\\pi}}(s) \\sum_{a} \\tilde{\\pi}(a~|~s) A_{\\pi}(s, a)\\\\\n",
    "    \\mu_{\\tilde{\\pi}}(s) & = \\mathbb{E}_{\\tilde{\\pi}}[\\sum_{t = 0}^{\\infty} \\gamma^{t} I(s_{t} = s)]\n",
    "    \\end{split}\n",
    "\\end{equation} \\hspace{1em} (Eq.~1)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d454d7b9",
   "metadata": {},
   "source": [
    "## 2.1. Local Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1f912",
   "metadata": {},
   "source": [
    "I copied over the text from the previous lecture for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975292d",
   "metadata": {},
   "source": [
    "We can slightly rewrite Eq. 4 so that we have a substitute for $\\mu_{\\tilde{\\pi}}$:\n",
    "\n",
    "$$\n",
    "L_{\\pi}(\\tilde{\\pi}) = V(\\theta) + \\sum_{s} \\mu_{\\pi}(s) \\sum_{a} \\tilde{\\pi}(a~|~s) A_{\\pi}(s, a) \\hspace{1em} (Eq.~2)\\\\\n",
    "$$\n",
    "\n",
    "Eq. 5, instead  of using the discounted weighted frequency of state $s$ under policy $\\mu_{\\tilde{\\pi}}$, uses $\\mu_{\\pi}$, the current policy's discounted weighted frequency of state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10d428",
   "metadata": {},
   "source": [
    "This begs the question: how do Eq. 3 and Eq. 4 fit into our current understanding of policy gradients? Over Lecture 8 and Lecture 9, we have seen a lot of formulas involving value functions.\n",
    "\n",
    "For now, I'm still not too sure. Let's give it some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cad738",
   "metadata": {},
   "source": [
    "## 2.2. Trust Region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a8182",
   "metadata": {},
   "source": [
    "Disclaimer: I won't cover too much of the theory!\n",
    "\n",
    "With our new formulation in Eq. 1, we want to ask: is there a bound to the new policy's performance by optimizing on the surrogate objective (the local approximation)?\n",
    "\n",
    "$$\n",
    "\\pi_{new}(a~|~s) = (1 - \\alpha) \\pi_{old}(a~|~s) + \\alpha \\pi'(a~|~s)\n",
    "$$\n",
    "\n",
    "Consider a __mixture policy__ (a blend of 2 policies), it will have a percentage of the old and a percentage of the new policy. For general stochastic policies we have the theorem (Eq. 3):\n",
    "\n",
    "$$\n",
    "D_{TV}^{max}(\\pi_{1}, \\pi_{2}) = \\underset{s}{max} D_{TV}(\\pi_{1}(\\cdot~|~s), \\pi_{2}(\\cdot~|~s))\\\\\n",
    "\\epsilon = \\underset{s}{max}[\\mathbb{E}_{a \\sim \\pi'(a~|~s)}[A_{\\pi}(s, a)]]\\\\\n",
    "D_{TV}(p, q)^{2} \\le D_{KL}(p, q)\\\\\n",
    "C = \\frac{4 \\epsilon \\gamma}{(1 - \\gamma)^{2}}\\\\\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "V^{\\pi_{new}} & \\ge L_{\\pi_{old}}(\\pi_{new}) - \\frac{4 \\epsilon \\gamma}{(1 - \\gamma)^{2}}(D_{TV}^{max}(\\pi_{old}, \\pi_{new}))^{2}\\\\\n",
    "    & \\ge L_{\\pi_{old}}(\\pi_{new}) - \\frac{4 \\epsilon \\gamma}{(1 - \\gamma)^{2}}D_{KL}^{max}(\\pi_{old}, \\pi_{new}) = M_{i}(\\pi)\n",
    "    \\end{split}\n",
    "\\end{equation} \\hspace{1em} (Eq.~3)\\\\\n",
    "V^{\\pi_{i + 1}} - V^{\\pi_{i}} \\ge M_{i}(\\pi_{i + 1}) - M_{i}(\\pi_{i}) \\hspace{1em} (Eq.~4)\\\\\n",
    "$$\n",
    "\n",
    "From the theorem (Eq. 3), we can derive Eq. 4. Eq. 4 simply says that we can have a monotonically improving general stochastic policy. For $C$, we tend to make this a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15c253",
   "metadata": {},
   "source": [
    "With the theorem established, we can put it into practice. Let's formulate our objective function: \n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{max} L_{\\pi_{old}}(\\pi_{new}) - CD_{KL}^{max}(\\pi_{old}, \\pi_{new}) \\hspace{1em} (Eq.~4)\\\\\n",
    "$$\n",
    "\n",
    "We can rewrite this to:\n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{max} L_{\\pi_{old}}(\\pi_{new}) \\hspace{1em} (Eq.~5)\\\\\n",
    "subject ~ to ~ D_{KL}^{s \\sim \\mu_{\\theta_{old}}}(\\pi_{old}, \\pi_{new}) \\le \\delta\n",
    "$$\n",
    "\n",
    "This formulation leverages not only the previous objective function in local approximation, but also the new lower bound theorem. The constraint serves as a __trust region__ for the KL divergence between the old and new policies.\n",
    "\n",
    "We make 3 substitutions (note $\\tilde{\\pi} = \\pi_{new} = \\theta_{new}$ and the same goes for the old):\n",
    "\n",
    "1. Substituting $\\sum_{s} \\mu_{\\theta_{old}}(s)$\n",
    "\n",
    "$$\n",
    "L_{\\pi_{old}} = V(\\theta) + \\sum_{s} \\mu_{\\tilde{\\pi}}(s) \\sum_{a} \\tilde{\\pi}(a~|~s) A_{\\pi}(s, a)\\\\\n",
    "becomes\\\\\n",
    "L_{\\pi_{old}} = V(\\theta) + \\frac{1}{1 - \\gamma} \\mathbb{E}_{s \\sim \\mu_{\\theta_{old}}} [\\sum_{a} \\tilde{\\pi}(a~|~s) A_{\\pi}(s, a)] \\hspace{1em} (Eq.~5)\\\\\n",
    "$$\n",
    "\n",
    "This substitution is made because our state space can be continuous and infinite so summing over it would be impossible in practice.\n",
    "\n",
    "2. Substituting $\\sum_{a} \\tilde{\\pi}(a~|~s) A_{\\pi}(s, a)$\n",
    "\n",
    "$$\n",
    "(Eq.~5)\\\\\n",
    "becomes\\\\\n",
    "L_{\\pi_{old}} = V(\\theta) + \\frac{1}{1 - \\gamma} \\mathbb{E}_{s \\sim \\mu_{\\theta_{old}}} [\\mathbb{E}_{a \\sim q}[\\frac{\\pi_{\\theta}(a~|~s_{n})}{q(a~|~s_{n})}A_{\\theta_{old}}(s_{n}, a)]] \\hspace{1em} (Eq.~6)\\\\\n",
    "$$\n",
    "\n",
    "Again, summing over actions can be a continuous set. So instead, we use importance sampling.\n",
    "\n",
    "3. Substituting $A_{\\theta_{old}}$\n",
    "\n",
    "$$\n",
    "(Eq.~6)\\\\\n",
    "becomes\\\\\n",
    "\\underset{\\theta}{max} \\mathbb{E}_{s \\sim \\mu_{\\theta_{old}}, a \\sim q} [\\frac{\\pi_{\\theta}(a~|~s)}{q(a~|~s)}Q_{\\theta_{old}}(s, a)] \\hspace{1em} (Eq.~7)\\\\\n",
    "subject ~ to ~ \\mathbb{E}_{s \\sim \\mu_{\\theta_{old}}} D_{KL}(\\pi_{old}(\\cdot~|~s), \\pi_{new}(\\cdot~|~s)) \\le \\delta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24662685",
   "metadata": {},
   "source": [
    "## 2.3. TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e6565",
   "metadata": {},
   "source": [
    "for iteration = 1,2, ... do <br>\n",
    "$\\quad$ Run policy for $T$ timesteps or $N$ trajectories <br>\n",
    "$\\quad$ Estimate advantage function at all time steps <br>\n",
    "$\\quad$ Compute policy gradient $g$ <br>\n",
    "$\\quad$ Use CG to compute $F^{-1}$g where $F$ is the Fisher information matrix <br>\n",
    "$\\quad$ Do line search on surrogate loss and KL constraint <br>\n",
    "\n",
    "_Algorithm 1. Trust Region Policy Optimization (TRPO)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61137305",
   "metadata": {},
   "source": [
    "Algorithm 1 is just a brief look into TRPO! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f657f6b",
   "metadata": {},
   "source": [
    "# 3. Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e04a27",
   "metadata": {},
   "source": [
    "If you missed the link right below the title, I'm providing the resource here again along with the course website.\n",
    "\n",
    "- [Stanford CS234](https://www.youtube.com/watch?v=FgzM3zpZ55o)\n",
    "- [Course Website](http://web.stanford.edu/class/cs234/index.html)\n",
    "\n",
    "This is a series of 15 lectures provided by Stanford.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c15c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
