{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa086de2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f95bc92081f655226a1fc9d54e09b1a4",
     "grade": false,
     "grade_id": "cell-2175775ae31eceae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Deep Learning Coding Project 1: Backpropogation\n",
    "\n",
    "Before we start, please put your **Chinese** name and student ID in following format:\n",
    "\n",
    "Name, 0000000000 // e.g.) 张三, 2021123123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b760d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e30c80977fd0505d51c0f3fff9f44141",
     "grade": true,
     "grade_id": "name-and-id",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827d07c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8c52ec82c784c8f7741f8adb4b6e713",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will use Python 3 throughout this course. Besides, we recommend using [NumPy](https://numpy.org/) and [PyTorch](https://pytorch.org/) packages for implementation, and assignments will be tested under the latest stable release version.\n",
    "\n",
    "In this notebook, you will derive and implement the gradient of several common functions/modules used in deep learning. We note that you are **NOT** allowed to use any autograd framework specially in this notebook, e.g. Tensorflow, PyTorch, JAX, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258cd51",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ddb6524bc547c239e1255ef1e1b2958",
     "grade": false,
     "grade_id": "cell-bd3ebc430e290b79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In some cells and files you will see code blocks that look like this:\n",
    "\n",
    "```Python\n",
    "##############################################################################\n",
    "#                  TODO: You need to complete the code here                  #\n",
    "##############################################################################\n",
    "raise NotImplementedError()\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################\n",
    "```\n",
    "\n",
    "You should replace `raise NotImplementedError()` with your own implementation based on the context, such as:\n",
    "\n",
    "```Python\n",
    "##############################################################################\n",
    "#                  TODO: You need to complete the code here                  #\n",
    "##############################################################################\n",
    "y = w * x + b\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################\n",
    "\n",
    "```\n",
    "\n",
    "When completing the notebook, please adhere to the following rules:\n",
    "\n",
    "+ Do not write or modify any code outside of code blocks\n",
    "+ Do not add or delete any cells from the notebook.\n",
    "+ Run all cells before submission. We will not re-run the entire codebook during grading.\n",
    "\n",
    "This notebook contains many inline sanity checks for the code you write. However, passing these sanity checks does not mean your code is correct! During grading we may run your code on additional inputs, and we may manually check your code to ensure correctness.\n",
    "\n",
    "**Finally, avoid plagiarism! Any student who violates academic integrity will be seriously dealt with and receive an F for the course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32460e0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6510d2810afc0ff3ec43f71f58a56076",
     "grade": false,
     "grade_id": "cell-2c3559630411672b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup Code\n",
    "\n",
    "You have already learned how to do backpropagation for a linear layer in the lecture. Now you can try to derive the backpropagation for some other layers by yourself.\n",
    "\n",
    "Let’s denote the loss function as $L$, the input to each layer as $z$ and the\n",
    "output of each layer as $y$. We additionally assume that the gradient of the loss\n",
    "function with respect to the output of the layer, i.e., $\\frac{dL}{dy}$, is given.\n",
    "Your goal is to derive:\n",
    "1. The gradient of the loss function with respect to the input of the layer,\n",
    "i.e., $\\frac{dL}{dz}$;\n",
    "2. (For linear layer and convolutional layer.) The gradient of the loss function with\n",
    "respect to all trainable parameters, i.e., $\\frac{dL}{d\\textrm{weight}}$ and $\\frac{d L}{d \\textrm{bias}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c2354b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b34bc8b23f9c8e480a9671ef3453e7ac",
     "grade": false,
     "grade_id": "cell-a551fcc5ff27fb87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca3738",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77a23475ef9ff40507ad44173ee87e22",
     "grade": false,
     "grade_id": "cell-291232b1c59e4f02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you use Colab in this coding project, please uncomment the code, fill the `GOOGLE_DRIVE_PATH_AFTER_MYDRIVE` and run the following cells to mount your Google drive. Then, the notebook can find the required file (i.e., module.py). If you run the notebook locally, you can skip the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca391e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# # Example: If you create a 2022SP folder and put all the files under CP1 folder, then '2022SP/CP1'\n",
    "# # GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2022SP/CP1'\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None \n",
    "# GOOGLE_DRIVE_PATH = os.path.join('drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "# print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd2080",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1568d67f2a87482f90f0d376a3303db1",
     "grade": false,
     "grade_id": "cell-e11eaf041d72deda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from module import hello\n",
    "hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb022ac4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26eacaaf972d848404921e45b1d3a53c",
     "grade": false,
     "grade_id": "cell-4c38f3276037d05b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, please run the following cell to import some base classes for implementation (no matter whether you use colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791b2fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3688ac9d3f35179dc98fc79e7ad81542",
     "grade": false,
     "grade_id": "cell-b763263889f471b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from module import Zeros, XavierUniform, Activation, module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847e605",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71fb332a00562ca99654ea85f89aa0e7",
     "grade": false,
     "grade_id": "cell-3e0b706b0545e331",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part A: Backpropation (7 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1424b15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfc01048b851f72e4f3a5db59e34efef",
     "grade": false,
     "grade_id": "cell-621b5ab2213c50b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A.1: Linear Layer (2 pts)\n",
    "\n",
    "We have provided the *_forward* function\n",
    "of the linear module and you need to implement their *_backward* functions following your derivation. You need to return the gradient of inputs, and save the gradient of weight into `self.grads[“weight”]`, the gradient of bias into `self.grads[“bias”]`. Note that your implementation should support both batched and unbatched inputs, similar to `torch.nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b363c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c160bccc330784a6267352e9ec612fa",
     "grade": false,
     "grade_id": "cell-2ba81560f305564f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Linear(module):\n",
    "    def __init__(self, d_in, d_out, w_init=XavierUniform(), b_init=Zeros()):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.initializers = {\n",
    "            \"weight\": w_init,\n",
    "            'bias': b_init,\n",
    "        }\n",
    "\n",
    "        self.input = None\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "\n",
    "        if d_in:\n",
    "            self.shapes = {\n",
    "                \"weight\": [d_in, d_out],\n",
    "                \"bias\": [d_out]\n",
    "            }\n",
    "\n",
    "            self._init_params()\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        if not self.is_init:\n",
    "            d_in = inputs.shape[-1]\n",
    "            self.shapes = {\n",
    "                \"weight\": [d_in, self.d_out],\n",
    "                \"bias\": [self.d_out]\n",
    "            }\n",
    "            self.d_in = d_in\n",
    "            self._init_params()\n",
    "\n",
    "        # `@` is the matrix multiplication operator in NumPy\n",
    "        out = inputs @ self.params['weight'] + self.params['bias']\n",
    "        self.input = inputs\n",
    "        return out\n",
    "\n",
    "    def _backward(self, grad):\n",
    "        ##############################################################################\n",
    "        #                  TODO: You need to complete the code here                  #\n",
    "        ##############################################################################\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ##############################################################################\n",
    "        #                              END OF YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "    @property\n",
    "    def param_names(self):\n",
    "        return ('weight', 'bias')\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.params['weight']\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self.params['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f56dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a3242302c576d271467a75a525796f9",
     "grade": true,
     "grade_id": "cell-23d28f782174e8e3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "    [0.41259363, -0.40173373, -0.9616683, 0.32021663, 0.30066854], dtype=np.float32)\n",
    "w = np.array([[-0.29742905, -0.4652604,  0.03716598],\n",
    "              [0.63429886,  0.46831214,  0.22899507],\n",
    "              [0.7614463,  0.45421863, -0.7652458],\n",
    "              [0.6237591,  0.71807355,  0.81113386],\n",
    "              [-0.34458044,  0.094055,  0.70938754]], dtype=np.float32)\n",
    "b = np.array([0., 0., 0.], dtype=np.float32)\n",
    "\n",
    "l = Linear(5, 3)\n",
    "l.params['weight'] = w\n",
    "l.params['bias'] = b\n",
    "l.is_init = True\n",
    "y = l._forward(x)\n",
    "l._backward(y)\n",
    "\n",
    "grad_b = np.array([-1.0136619, -0.5586895,  1.1322811], dtype=np.float32)\n",
    "grad_w = np.array([[-0.41823044, -0.23051172,  0.46717197],\n",
    "                   [0.40722215,  0.2244444, -0.4548755],\n",
    "                   [0.9748065,  0.53727394, -1.0888789],\n",
    "                   [-0.32459137, -0.17890166,  0.36257523],\n",
    "                   [-0.30477622, -0.16798034,  0.3404413]], dtype=np.float32)\n",
    "\n",
    "assert (np.abs(l.grads['bias'] - grad_b) < 1e-5).all()\n",
    "assert (np.abs(l.grads['weight'] - grad_w) < 1e-5).all()\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c5fa0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aca6ee2e79f5f2ade726b65122142e22",
     "grade": false,
     "grade_id": "cell-23fda2f4180d2cf8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A.2: 2D Convolutional Layer (2 pts)\n",
    "\n",
    "Suppose your input tensor $z$ has shape $(C_{in},H_{in},W_{in})$ and output tensor $y$ has shape $(C_{out},H_{out},W_{out})$, where $C$ denotes the number of channels and $H$, $W$ denote the height and width of the images. A convolutional layer computes\n",
    "\n",
    "$$y(j) = \\text{bias}(j) +\n",
    "\\sum_{k = 0}^{C_{in} - 1} \\text{weight}(j, k) \\star z(k),\\,\\,\\,\\,\\text{for}\\,\\,0\\le j \\le C_{out}-1$$\n",
    "\n",
    "where \"$\\text{weight}$\" is the convolutional kernal with size $(C_{out},C_{in},k_H,k_W)$, \"$\\text{bias}$\" is of size $(C_{out},)$, and \"$\\star$\" denotes the 2D convolution operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6915224",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c862ce970f1b6c918854964d05b52510",
     "grade": false,
     "grade_id": "cell-db2147f1947f238e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Conv2D(module):\n",
    "    \"\"\"\n",
    "    Implement 2D convolution layer\n",
    "    :param kernel: A list/tuple of int that has length 4 (in_channels, height, width, out_channels)\n",
    "    :param stride: A list/tuple of int that has length 2 (height, width)\n",
    "    :param padding: String [\"SAME\", \"VALID\"]\n",
    "    :param w_init: weight initializer\n",
    "    :param b_init: bias initializer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 kernel,\n",
    "                 stride=(1, 1),\n",
    "                 padding=\"SAME\",\n",
    "                 w_init=XavierUniform(),\n",
    "                 b_init=Zeros()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_shape = kernel\n",
    "        self.stride = stride\n",
    "        self.initializers = {\"weight\": w_init, \"bias\": b_init}\n",
    "        self.shapes = {\"weight\": self.kernel_shape,\n",
    "                       \"bias\": self.kernel_shape[-1]}\n",
    "\n",
    "        self.padding_mode = padding\n",
    "        assert padding in ['SAME', 'VALID']\n",
    "        if padding == 'SAME' and stride != (1, 1):\n",
    "            raise RunTimeError(\n",
    "                \"padding='SAME' is not supported for strided convolutions.\")\n",
    "        self.padding = None\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs:  shape (batch_size, in_c, in_h, in_w)\n",
    "        :return outputs: shape (batch_size, out_c, out_h, out_w)\n",
    "        where batch size is the number of images\n",
    "        \"\"\"\n",
    "        assert len(\n",
    "            inputs.shape) == 4, 'Expected shape of inputs is (batch_size, in_c, in_h, in_w).'\n",
    "        in_c, k_h, k_w, out_c = self.kernel_shape\n",
    "        s_h, s_w = self.stride\n",
    "        X = self._inputs_preprocess(inputs)\n",
    "        bsz, _, h, w = X.shape\n",
    "\n",
    "        out_h = (h - k_h) // s_h + 1\n",
    "        out_w = (w - k_w) // s_w + 1\n",
    "        Y = np.zeros([bsz, out_c, out_h, out_w])\n",
    "        for in_c_i in range(in_c):\n",
    "            for out_c_i in range(out_c):\n",
    "                kernel = self.params['weight'][in_c_i, :, :, out_c_i]\n",
    "                for r in range(out_h):\n",
    "                    r_start = r * s_h\n",
    "                    for c in range(out_w):\n",
    "                        c_start = c * s_w\n",
    "                        patch = X[:, in_c_i, r_start: r_start +\n",
    "                                  k_h, c_start: c_start+k_w] * kernel\n",
    "                        Y[:, out_c_i, r,\n",
    "                            c] += patch.reshape(bsz, -1).sum(axis=-1)\n",
    "        self.input = inputs\n",
    "        return Y + self.params['bias'].reshape(1, -1, 1, 1)\n",
    "\n",
    "    def _backward(self, grad):\n",
    "        \"\"\"\n",
    "        Compute gradients w.r.t layer parameters and backward gradients.\n",
    "        :param grad: gradients from previous layer \n",
    "            with shape (batch_size, out_c, out_h, out_w)\n",
    "        :return d_in: gradients to next layers \n",
    "            with shape (batch_size, in_c, in_h, in_w)\n",
    "        \"\"\"\n",
    "        assert len(\n",
    "            grad.shape) == 4, 'Expected shape of upstream gradient is (batch_size, out_c, out_h, out_w)'\n",
    "        ##############################################################################\n",
    "        #                  TODO: You need to complete the code here                  #\n",
    "        ##############################################################################\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ##############################################################################\n",
    "        #                              END OF YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "    def _inputs_preprocess(self, inputs):\n",
    "        _, _, in_h, in_w = inputs.shape\n",
    "        _, k_h, k_w, _ = self.kernel_shape\n",
    "        # padding calculation\n",
    "        if self.padding is None:\n",
    "            self.padding = self.get_padding_2d(\n",
    "                (in_h, in_w), (k_h, k_w), self.stride, self.padding_mode)\n",
    "        return np.pad(inputs, pad_width=self.padding, mode=\"constant\")\n",
    "\n",
    "    def get_padding_2d(self, in_shape, k_shape, stride, mode):\n",
    "\n",
    "        def get_padding_1d(w, k, s):\n",
    "            if mode == \"SAME\":\n",
    "                pads = s * (w - 1) + k - w\n",
    "                half = pads // 2\n",
    "                padding = (half, half) if pads % 2 == 0 else (half, half + 1)\n",
    "            else:\n",
    "                padding = (0, 0)\n",
    "            return padding\n",
    "\n",
    "        h_pad = get_padding_1d(in_shape[0], k_shape[0], stride[0])\n",
    "        w_pad = get_padding_1d(in_shape[1], k_shape[1], stride[1])\n",
    "        return (0, 0), (0, 0), h_pad, w_pad\n",
    "\n",
    "    @property\n",
    "    def param_names(self):\n",
    "        return \"weight\", \"bias\"\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.params['weight']\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self.params['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22357b1c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d96984b7fb7ceca321a6fc948825d7a",
     "grade": true,
     "grade_id": "cell-a2ba2f92ebfc0172",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[[[-1.75957111,  0.0085911,  0.30235818],\n",
    "                [-1.05931037,  0.75555462, -2.03922536],\n",
    "                [0.86653209, -0.56438439, -1.68797524]],\n",
    "\n",
    "               [[-0.74832044,  0.21611616,  0.571611],\n",
    "                [-1.61335018, -0.37620906,  1.0353189],\n",
    "                [-0.26074537,  1.98065489, -1.30691981]]],\n",
    "\n",
    "\n",
    "              [[[0.32680334,  0.29817393,  2.25433969],\n",
    "                [-0.16831957, -0.98864486,  0.36653],\n",
    "                  [1.52712821,  1.19630751, -0.02024759]],\n",
    "\n",
    "               [[0.48080474, -1.15229596, -0.95228854],\n",
    "                  [-1.68168285, -2.86668484, -0.34833734],\n",
    "                  [0.73179971,  1.69618114,  1.33524773]]]], dtype=np.float32)\n",
    "w = np.array([[[[-0.322831,  0.38674766,  0.32847992,  0.3846352],\n",
    "                [-0.21158722, -0.53467643, -0.28443742, -0.20367976]],\n",
    "\n",
    "               [[0.4973593, -0.30178958, -0.02311361, -0.53795236],\n",
    "                [-0.1229187, -0.12866518, -0.40432686,  0.5104686]]],\n",
    "\n",
    "\n",
    "              [[[0.19288206, -0.49516755, -0.26484585, -0.35625377],\n",
    "                [0.5058061, -0.17490079, -0.40337119,  0.10058666]],\n",
    "\n",
    "               [[-0.24815331,  0.34114942, -0.06982624,  0.4017606],\n",
    "                  [0.16874631, -0.42147416,  0.43324274,  0.16369782]]]], dtype=np.float32)\n",
    "b = np.array([0., 0., 0., 0.], dtype=np.float32)\n",
    "\n",
    "l = Conv2D(kernel=(2, 2, 2, 4), padding='SAME', stride=(1, 1))\n",
    "l.params['weight'] = w\n",
    "l.params['bias'] = b\n",
    "l.is_init = True\n",
    "y = l._forward(x)\n",
    "l._backward(y)\n",
    "\n",
    "grad_b = np.array([-0.49104962,  1.4335476,  2.70048173, -\n",
    "                  0.0098734], dtype=np.float32)\n",
    "grad_w = np.array([[[[-3.0586028,   7.7819834,   1.3951588,   5.9249396],\n",
    "                     [-1.5760803, -10.541515,  -2.694372,  -3.9848034]],\n",
    "\n",
    "                    [[2.9096646,   0.6696263,   8.230143,  -0.3434674],\n",
    "                     [-2.9487448,  -3.264796,  -1.1822633,   4.1672387]]],\n",
    "\n",
    "\n",
    "                   [[[3.7202294,  -5.4176836, -10.34358,  -6.4479938],\n",
    "                     [7.0336857,  -0.41946477,  -8.181945,   3.0968976]],\n",
    "\n",
    "                    [[0.25020388,  13.39637,   5.8576417,  12.522377],\n",
    "                       [3.360495,  -6.597466,   8.375789,   3.8179488]]]], dtype=np.float32)\n",
    "\n",
    "assert (np.abs(l.grads['bias'] - grad_b) < 1e-5).all()\n",
    "assert (np.abs(l.grads['weight'] - grad_w) < 1e-5).all()\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bccd769",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "602e00de26a14a87dec3014b13edd6a7",
     "grade": false,
     "grade_id": "cell-fd1db8a11a63652e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A.3: 2D Max-Pooling Layer (1 pt)\n",
    "\n",
    "Suppose your input tensor $z$ has shape $(C,H_{in},W_{in})$ and output tensor $y$ has shape $(C,H_{out},W_{out})$, where $C$ denotes the number of channels and $H$, $W$ denote the height and width of the images. Suppose the kernel size of $(k_H,k_W)$. A max-pooling layer computes\n",
    "\n",
    "\\begin{aligned}\n",
    "    y(j, h, w) ={} & \\max_{m=0, \\ldots, k_H-1} \\max_{n=0, \\ldots, k_W-1} z(j, \\text{stride[0]} \\times h + m, \\text{stride[1]} \\times w + n)\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afc8a7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aece6a54601d6834169a9a007b057f87",
     "grade": false,
     "grade_id": "cell-cdc2f1d80735884e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool2D(module):\n",
    "\n",
    "    def __init__(self, pool_size, stride, padding=\"VALID\"):\n",
    "        \"\"\"\n",
    "        Implement 2D max-pooling layer\n",
    "        :param pool_size: A list/tuple of 2 integers (pool_height, pool_width)\n",
    "        :param stride: A list/tuple of 2 integers (stride_height, stride_width)\n",
    "        :param padding: A string (\"SAME\", \"VALID\")\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel_shape = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "        self.padding_mode = padding\n",
    "        self.padding = None\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs:  shape (batch_size, in_c, in_h, in_w)\n",
    "        \"\"\"\n",
    "        s_h, s_w = self.stride\n",
    "        k_h, k_w = self.kernel_shape\n",
    "        batch_sz, in_c, in_h, in_w = inputs.shape\n",
    "\n",
    "        # zero-padding\n",
    "        if self.padding is None:\n",
    "            self.padding = self.get_padding_2d(\n",
    "                (in_h, in_w), (k_h, k_w), self.stride, self.padding_mode)\n",
    "        X = np.pad(inputs, pad_width=self.padding, mode=\"constant\")\n",
    "        padded_h, padded_w = X.shape[2:4]\n",
    "\n",
    "        out_h = (padded_h - k_h) // s_h + 1\n",
    "        out_w = (padded_w - k_w) // s_w + 1\n",
    "\n",
    "        # construct output matrix and argmax matrix\n",
    "        max_pool = np.empty(shape=(batch_sz, in_c, out_h, out_w))\n",
    "        argmax = np.empty(shape=(batch_sz, in_c, out_h, out_w), dtype=int)\n",
    "        for r in range(out_h):\n",
    "            r_start = r * s_h\n",
    "            for c in range(out_w):\n",
    "                c_start = c * s_w\n",
    "                pool = X[:, :, r_start: r_start+k_h, c_start: c_start+k_w]\n",
    "                pool = pool.reshape((batch_sz, in_c, -1))\n",
    "\n",
    "                _argmax = np.argmax(pool, axis=2)[:, :, np.newaxis]\n",
    "                argmax[:, :, r, c] = _argmax.squeeze(axis=2)\n",
    "\n",
    "                # get max elements\n",
    "                _max_pool = np.take_along_axis(\n",
    "                    pool, _argmax, axis=2).squeeze(axis=2)\n",
    "                max_pool[:, :, r, c] = _max_pool\n",
    "\n",
    "        self.X_shape = X.shape\n",
    "        self.out_shape = (out_h, out_w)\n",
    "        self.argmax = argmax\n",
    "        return max_pool\n",
    "\n",
    "    def _backward(self, grad):\n",
    "        ##############################################################################\n",
    "        #                  TODO: You need to complete the code here                  #\n",
    "        ##############################################################################\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ##############################################################################\n",
    "        #                              END OF YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "    def get_padding_2d(self, in_shape, k_shape, stride, mode):\n",
    "\n",
    "        def get_padding_1d(w, k, s):\n",
    "            if mode == \"SAME\":\n",
    "                pads = s * (w - 1) + k - w\n",
    "                half = pads // 2\n",
    "                padding = (half, half) if pads % 2 == 0 else (half, half + 1)\n",
    "            else:\n",
    "                padding = (0, 0)\n",
    "            return padding\n",
    "\n",
    "        h_pad = get_padding_1d(in_shape[0], k_shape[0], stride[0])\n",
    "        w_pad = get_padding_1d(in_shape[1], k_shape[1], stride[1])\n",
    "        return (0, 0), (0, 0), h_pad, w_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1c600",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cf65234cd78d22ec4ae5e12cd5a134a",
     "grade": true,
     "grade_id": "cell-2f2417c24719b364",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[[[-2.23317337,  0.9750834, -1.30762567, -0.71442179],\n",
    "                [0.24624013, -1.77593893, -0.43530428,  1.03446008],\n",
    "                [1.58317228, -0.66459249,  0.54894879, -1.19706709],\n",
    "                [0.06013156,  1.05886458,  0.26634763,  1.03497421]]],\n",
    "\n",
    "\n",
    "              [[[2.20214308, -0.53358514,  0.96765812, -1.74976553],\n",
    "                [-0.07049627,  0.88147726,  2.15051543, -0.78627764],\n",
    "                  [1.19180886,  0.00468398, -1.74774108,  0.18564536],\n",
    "                  [1.39397303, -1.0462731,  0.4786774, -0.51543751]]]], dtype=np.float32)\n",
    "\n",
    "l = MaxPool2D(pool_size=(2, 2), stride=(2, 2))\n",
    "l.is_init = True\n",
    "y = l._forward(x)\n",
    "grad_ = l._backward(y)\n",
    "\n",
    "grad = np.array([[[[0., 0.9750834, 0., 0.],\n",
    "                   [0., 0., 0., 1.0344601],\n",
    "                   [1.5831723, 0., 0., 0.],\n",
    "                   [0., 0., 0., 1.0349742]]],\n",
    "\n",
    "\n",
    "                 [[[2.2021432, 0., 0., 0.],\n",
    "                   [0., 0., 2.1505153, 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [1.393973, 0., 0.4786774, 0.]]]], dtype=np.float32)\n",
    "\n",
    "assert (np.abs(grad - grad_) < 1e-5).all()\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5b318",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50b9617841870add1ba9a4b4796948b8",
     "grade": false,
     "grade_id": "cell-b2f652c91d642f1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A.4: Miscellaneous (1 pt)\n",
    "\n",
    "Reshaping is necessary for a typical convolutional neural network, because image features should be flattened and sent into an MLP for classification. Similarly, implement the *_backward* function of reshaping layer.\n",
    "\n",
    "Next, you need to implement the gradient function of a ReLU and a tanh activation function. The ReLU function is defined as\n",
    "\n",
    "$$y=\\max (0,z)$$\n",
    "\n",
    "and the tanh function is defined as\n",
    "\n",
    "$$y=\\frac{\\exp(z)-\\exp(-z)}{\\exp(z)+\\exp(-z)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fd702",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3a42999ecb31aab1aef4a07b6ef8c44",
     "grade": false,
     "grade_id": "cell-b0f33be65f354828",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Reshape(module):\n",
    "\n",
    "    def __init__(self, *output_shape):\n",
    "        super().__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.input_shape = None\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        self.input_shape = inputs.shape\n",
    "        return inputs.reshape(inputs.shape[0], *self.output_shape)\n",
    "\n",
    "    def _backward(self, grad):\n",
    "        ##############################################################################\n",
    "        #                  TODO: You need to complete the code here                  #\n",
    "        ##############################################################################\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ##############################################################################\n",
    "        #                              END OF YOUR CODE                              #\n",
    "        ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742fb0c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b957aaf15f1975f2931134707a9fe3e1",
     "grade": false,
     "grade_id": "cell-d5de0dcfdbe9d10d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "\n",
    "    def func(self, x):\n",
    "        return np.maximum(x, 0.0)\n",
    "\n",
    "    def _backward(self, grad):\n",
    "        ##############################################################################\n",
    "        #                  TODO: You need to complete the code here                  #\n",
    "        ##############################################################################\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ##############################################################################\n",
    "        #                              END OF YOUR CODE                              #\n",
    "        ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f59452",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2459271754447ed2086ba898cfe7c96",
     "grade": false,
     "grade_id": "cell-91571681ca6da674",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "\n",
    "    def func(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _backward(self, grad):\n",
    "        ##############################################################################\n",
    "        #                  TODO: You need to complete the code here                  #\n",
    "        ##############################################################################\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ##############################################################################\n",
    "        #                              END OF YOUR CODE                              #\n",
    "        ##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d30e8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "424c79c84547a181b1edd78851886024",
     "grade": false,
     "grade_id": "cell-9b3a9a6fef130d21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A.5 It's time to build your own network! (1 pt)\n",
    "\n",
    "We provide an example. Run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6bbc4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2aa7a37d6d254ac01089e098f861431",
     "grade": true,
     "grade_id": "cell-b6ea9d8c02c8d0f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Conv2D([2, 3, 3, 10]),\n",
    "    ReLU(),\n",
    "    MaxPool2D([3, 3], [2, 2]),\n",
    "    Reshape(-1),\n",
    "    Linear(10, 10),\n",
    "    Tanh(),\n",
    "]\n",
    "layers[0].params[\"weight\"] = np.array(\n",
    "    [[[[0.10034741, -0.10923018, -0.13951169,  0.02620922,\n",
    "       0.11209463, -0.03927368,  0.2455522,  0.09440251,\n",
    "      -0.00973911, -0.0551014],\n",
    "     [-0.08009744,  0.11698803, -0.03137447, -0.22489624,\n",
    "      -0.05207429,  0.12155709, -0.16216859, -0.16576429,\n",
    "       0.01611499,  0.01625606],\n",
    "     [0.06864581,  0.17847365,  0.11464144,  0.05670569,\n",
    "       0.11361383, -0.09042443, -0.07059199, -0.13879062,\n",
    "      -0.10536136,  0.06689657]],\n",
    "\n",
    "      [[-0.208334, -0.03386239, -0.03531212, -0.00322536,\n",
    "        -0.03788247, -0.09588832, -0.03761636,  0.20092505,\n",
    "        0.22685647,  0.00093809],\n",
    "     [0.06330945, -0.19632441, -0.09332216, -0.04350284,\n",
    "       0.18709384, -0.1274559, -0.00866532,  0.24800156,\n",
    "       0.0099521,  0.05766132],\n",
    "     [-0.1937654,  0.16667984,  0.05263836,  0.02301866,\n",
    "          -0.08030899, -0.10004608, -0.04238123,  0.09260008,\n",
    "          0.19176605,  0.00532325]],\n",
    "\n",
    "      [[0.08647767,  0.04389243,  0.06379496,  0.08922312,\n",
    "        0.17485274, -0.2128848,  0.13467704, -0.1309234,\n",
    "        -0.15617682,  0.03700767],\n",
    "     [-0.20649141,  0.19680719,  0.06499291,  0.11411078,\n",
    "          -0.2471389,  0.04823145,  0.02900326, -0.1741877,\n",
    "          -0.1771956,  0.09986747],\n",
    "     [-0.09256576,  0.09804958,  0.02777646, -0.05671893,\n",
    "          0.21713808,  0.17450929, -0.07283475, -0.23311245,\n",
    "          -0.09971547, -0.05200206]]],\n",
    "\n",
    "\n",
    "     [[[0.10468353,  0.25300628, -0.07359204,  0.13409732,\n",
    "       0.04759048,  0.09791245, -0.17818803, -0.05164933,\n",
    "       -0.13235886, -0.07995545],\n",
    "       [0.00670526,  0.08510415, -0.20128378, -0.18852185,\n",
    "       -0.0909241,  0.08251962,  0.17697941,  0.0272014,\n",
    "       0.18103799, -0.05881954],\n",
    "       [-0.0935763, -0.07443489, -0.16799624,  0.16809557,\n",
    "      -0.08239949,  0.02674822,  0.04012047,  0.01099809,\n",
    "      -0.25400403,  0.24942434]],\n",
    "\n",
    "        [[0.2070298, -0.14932613, -0.10598685,  0.01022026,\n",
    "          0.20527782,  0.24701637, -0.12383634,  0.03287163,\n",
    "          0.15678546, -0.05395091],\n",
    "         [0.11802146, -0.17311034,  0.05143219,  0.1868667,\n",
    "            0.24696055, -0.21484058, -0.03659691, -0.15090589,\n",
    "            -0.02521261,  0.02439543],\n",
    "         [-0.20770998, -0.10375416,  0.21839033,  0.03524392,\n",
    "            -0.02175199,  0.12948939,  0.12353204, -0.23056503,\n",
    "            0.10659301,  0.17326987]],\n",
    "\n",
    "        [[-0.17062354,  0.1435208, -0.10902726, -0.09884633,\n",
    "          0.08440794, -0.19848298,  0.08420925,  0.19809937,\n",
    "          0.10026675, -0.03047777],\n",
    "         [-0.03155725,  0.13539886,  0.03352691, -0.21201183,\n",
    "            0.04222458,  0.16080765, -0.08321898,  0.21838641,\n",
    "            0.1280547,  0.03782839],\n",
    "         [0.12852815, -0.21495132,  0.18355937,  0.16420949,\n",
    "            0.20934355, -0.18967807, -0.21360746, -0.18468067,\n",
    "            -0.05139272, -0.03866057]]]], dtype=np.float32)\n",
    "\n",
    "layers[4].params['weight'] = np.array(\n",
    "    [[0.06815682, -0.41381145, -0.32710046,  0.34138927, -0.03506786,\n",
    "     0.33732942, -0.5395874,  0.056517,  0.47315797,  0.0900187],\n",
    "     [-0.321956,  0.23854145, -0.13256437,  0.18445538, -0.51560444,\n",
    "     0.14887139, -0.51245147,  0.26814377, -0.02967232, -0.41434735],\n",
    "     [0.04670532, -0.47457483,  0.1680028,  0.54343534,  0.29511,\n",
    "     0.08081549, -0.43529126,  0.21890727,  0.17655055, -0.49393934],\n",
    "     [0.32019785,  0.020503, -0.08120787,  0.31569323, -0.09687106,\n",
    "     -0.02078467, -0.34875813, -0.19573534,  0.37851244, -0.34297976],\n",
    "     [-0.09060311,  0.53571045, -0.28854045,  0.45661694,  0.45833147,\n",
    "        -0.44771242, -0.03981645,  0.00242787, -0.20411544, -0.4958647],\n",
    "        [-0.2829692, -0.4430751, -0.28673285,  0.33716825,  0.43267703,\n",
    "         -0.50037426, -0.21695638,  0.5264514,  0.04327536,  0.13836497],\n",
    "        [-0.54164785, -0.01653088,  0.5349371, -0.13672741, -0.44142258,\n",
    "         -0.04172686,  0.507196, -0.17326587,  0.32745343,  0.32736975],\n",
    "        [-0.319598, -0.06203758,  0.23617937, -0.09802067, -0.3384849,\n",
    "         0.51211435,  0.16513875,  0.4003412, -0.5200709, -0.2553419],\n",
    "     [0.00226878, -0.47383627,  0.54009086, -0.28869098, -0.13770601,\n",
    "     -0.31328425, -0.4322124, -0.29305372, -0.21842065,  0.14727412],\n",
    "        [-0.23964529, -0.15086825, -0.5412125, -0.14709733,  0.03712023,\n",
    "         -0.3702431,  0.10673262, -0.22659011,  0.14465407, -0.5190256]],\n",
    "    dtype=np.float32)\n",
    "\n",
    "x = np.array(\n",
    "    [[[[-0.8492061,  1.0779045, -0.08430817, -0.16879],\n",
    "     [-0.21486154, -1.3411552,  0.64358956,  0.09206003],\n",
    "     [-1.0160062,  0.75887114,  1.7270764,  0.12690294],\n",
    "     [-0.10278344, -0.57746404, -0.16129336, -0.42453188]],\n",
    "\n",
    "        [[-0.51087075,  1.566093, -0.35502744, -1.0280341],\n",
    "         [-0.44904083, -0.91717184, -0.2204062, -1.7094339],\n",
    "         [-0.20429765,  0.96035904,  1.1546314, -0.55592376],\n",
    "         [-3.0869954, -0.13439347,  1.047694, -0.16260263]]]],\n",
    "    dtype=np.float32)\n",
    "\n",
    "for l in layers:\n",
    "    x = l._forward(x)\n",
    "\n",
    "y = x\n",
    "\n",
    "e = np.array(\n",
    "    [[0.32257673, -0.43416652,  1.0324798, -0.19434273,  0.59407026,\n",
    "      -0.19911239,  0.2908744,  0.27966267,  0.24996994, -0.97430784]],\n",
    "    dtype=np.float32)\n",
    "\n",
    "for l in reversed(layers):\n",
    "    e = l._backward(e)\n",
    "\n",
    "dx = e\n",
    "\n",
    "target = np.array(\n",
    "    [[[[0.05311276,  0.0012092, -0.01387816, -0.09388599],\n",
    "     [-0.07132251,  0.00879665,  0.15649047, -0.09754436],\n",
    "     [-0.01795623,  0.00153671, -0.08608256,  0.04023483],\n",
    "     [0.01426923, -0.07778768, -0.03364746, -0.02747378]],\n",
    "\n",
    "        [[-0.07534513, -0.10424455, -0.00875467,  0.04870174],\n",
    "         [0.01257615, -0.1349132, -0.05684724, -0.09846888],\n",
    "         [-0.0293199,  0.01961213, -0.02899574,  0.17384738],\n",
    "         [-0.03952359,  0.0342231,  0.14877939, -0.00817257]]]],\n",
    "    dtype=np.float32)\n",
    "\n",
    "if dx.shape == target.shape and np.abs(target - dx).max() < 1e-4:\n",
    "    print(\"success!\")\n",
    "else:\n",
    "    print(\"fail!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b040898",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e95b412cbc60f570ab7db440f92c3b2",
     "grade": false,
     "grade_id": "cell-76e380c7c9e450ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part B: Get Your Hand Dirty (3 pts)\n",
    "\n",
    "In this problem, you need to train a neural network with different hyperparameters to solve the spiral classification problem in your browser: https://playground.tensorflow.org/#dataset=spiral, and answer the following questions. Please include necessary visualizations for all your attempts to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e3981",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a76df1394be450526c4d2c3abdbae7dc",
     "grade": false,
     "grade_id": "cell-8f0388009d461e92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### B.1 Show Your Results\n",
    "\n",
    "List your best set of hyper-parameters and show us your best result, how\n",
    "do you find this configuration?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c895e18",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43aa143ff47dcd8c66d8bb979d50e122",
     "grade": true,
     "grade_id": "cell-02c14ebda4d2b0cc",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaeca4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bf263e6bcc439c01d8f0752a89c0800",
     "grade": false,
     "grade_id": "cell-7216cfd14baf8be0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### B.2 Show Your Findings\n",
    "\n",
    "List your findings that how the learning rate, the number of hidden sizes,\n",
    "and the regularization influence the performance and convergence rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522400b3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcf8c42aba7e8713bf0f0d61e2152b77",
     "grade": true,
     "grade_id": "cell-d4076bbf3ee870f7",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
